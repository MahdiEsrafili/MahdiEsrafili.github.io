<h1 dir = 'rtl'> Ensemble Methods </h1>
<h2 dir = 'rtl'> Bagging </h2>
<p dir = 'rtl'> 
در این روش میایم چند تا مدل ضعیف می سازیم بصورت موازی، بعدش میایم از نتایج این ها نظرسنجی میکنیم یا اینکه میانگین میگیریم (برای رگرسیون).<br>
در اینجا هر کدوم از مدل های ضعیف، واریانس زیاد و بایاس کم دارند، وقتی ازشون میانگین میگیریم، واریانسشون کمتر میشه. در کل انگار هدف بگینگ کم کردن واریانس هست. یکی از روش های بگینگ، <a href="random_forest.md"> رندم فارست </a> هست.
</p>
<h2 dir = 'rtl'>Boosting </h2>

<p dir ='rtl'>
بوستینگ میگه بجای اینکه وقتمون رو بذاریم برای ساخت یه مدل قوی و دقیق، بیایم چند تا مدل ضعیف رو ترکیب کنیم، یعنی از هوش جمعی 
استفاده کنیم بجای هوش تکی. 
یکی از الگوریتم های بوستینگ، Adaboost هست که برای کلاس بندیه و Gradient Boosting هم برای رگرسیون. مدلشون اینطوریکه در هر مرحله، بعد از مدل سازی و پیش بینی، داده هایی که اشتباه کلاس بندی شدن، وزن بیشتری میگیرن برای مدل بعدی.پیاده سازی ها توی پکیج sklearn هست.
 </p>

<h2 dir ='rtl'> Stacking</h2>
<p dir ='rtl'>
در روش های قبلی، چند تا مدل ضعیف (weak learner) از یک نوع انتخاب میکردیم؛ یعنی همشون یا درخت تصمیم بودن یا بیزین و ... . ولی در این روش، مدل های ضعیف از چند نوع مختلف هستن، یعنی یکیش درخت تصمیم، اون یکی بیزین، KNN، SVM و ... . بعد اینکه این مدل ها رو آموزش دادیم، یک متامدل دیگه مثل شبکه عصبی داریم که داده های آموزشیش، نتایج پیش بینی مدل های ضعیفه. یک روش پیشرفته تر, استفاده از چند متامدل هست، در واقع بجای استفاده از یک متامدل، چند تا متامدل رو آموزش بدیم و سپس از یه متامدل دیگه برای آموزش روی نتایج متامدل های قبلی استفاده بکنیم.
 </p>