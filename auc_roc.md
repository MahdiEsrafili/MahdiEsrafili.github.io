<h1 dir = 'rtl'> نمودارهای ROC</h1>
<p dir = 'rtl'>
اول از همه، دو تا نرخ مهم داریم : <br>
TPR: یعنی تعداد مثبت هایی که درست تشخیص دادیم به کل مثبت ها. یعنی از کل مثبت ها چقدرشون رو مثبت پیش بینی کردیم.<br>
FPR: یعنی تشخیص های مثبت اشتباه نسبت به همه منفی ها. یعنی از کل منفی ها چقدرشون رو به اشتباه مثبت تشخیص دادیم.<br>
این نمودار برای انتخاب بهترین حد آستانه در مسائل کلاس بندی هست. یعنی اول میایم مدل رو بر اساس داده های آموزشی درست میکنیم، بعد میایم احتمال داده های تست برای هر کلاس رو بدست میاریم. بعد میایم به ازای حد آستانه های مختلف برای این احتمالات، نمودار میکشیم.
</p>

![](ROC_AUC/ROC.png)

<p dir = 'rtl'>
شکل بالا میگه اون خط آبی انگار چیزی نمیده و بصورت رندم داده رو لیبل میزنه، یعنی به احتمال 0.5 درست تشخیص بده.<br>
AUC یعنی مساحت زیر منحنی، هر چقدر زیاد باشه یعنی مدل میتونه خوب پیش بینی کنه.<br>
علاوه بر این نمودار، یه نمودار بر حسب Precision/Recall هم داریم. نمودار ROC برای داده های بالانس و این یکی نمودار برای داده های نابالانس هست.<br>
AUC هم یعنی مساحت زیر نمودار، هر چقدر بیشتر بشه، یعنی اینکه مدل به حالت ایده آل نزدیکتره.
<br>
به fp خطای نوع یک و به fn خطای نوع دو میگن!
</p>
<pre><code>
Recall = TP / (TP + FN)
Precision = TP / (TP + FP)
F-Score = (2 * Recall * Precision) / (Recall + Precision)
</code></pre>
<p dir = 'rtl'>
Precision: یعنی از کل مثبت هایی که تشخیص داده، چقدرشون واقعا مثبته و درست تشخیص داده شده؟<br>
Recall: یعنی مدل چقدر تونسته مثبت ها رو شناسایی کنه
</p>

[notebook](https://github.com/MahdiEsrafili/MahdiEsrafili.github.io/blob/master/ROC_AUC/ROC_AUC.ipynb)


